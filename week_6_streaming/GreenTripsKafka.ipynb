{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186f68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "import time \n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import types\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9735a339",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ecd26ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " load green trips data to df..\n",
      "1 {'VendorID': 2.0, 'lpep_pickup_datetime': '2019-10-01 00:26:02', 'lpep_dropoff_datetime': '2019-10-01 00:39:58', 'store_and_fwd_flag': 'N', 'RatecodeID': 1.0, 'PULocationID': 112, 'DOLocationID': 196, 'passenger_count': 1.0, 'trip_distance': 5.88, 'fare_amount': 18.0, 'extra': 0.5, 'mta_tax': 0.5, 'tip_amount': 0.0, 'tolls_amount': 0.0, 'ehail_fee': nan, 'improvement_surcharge': 0.3, 'total_amount': 19.3, 'payment_type': 2.0, 'trip_type': 1.0, 'congestion_surcharge': 0.0} \n",
      "2 {'VendorID': 1.0, 'lpep_pickup_datetime': '2019-10-01 00:18:11', 'lpep_dropoff_datetime': '2019-10-01 00:22:38', 'store_and_fwd_flag': 'N', 'RatecodeID': 1.0, 'PULocationID': 43, 'DOLocationID': 263, 'passenger_count': 1.0, 'trip_distance': 0.8, 'fare_amount': 5.0, 'extra': 3.25, 'mta_tax': 0.5, 'tip_amount': 0.0, 'tolls_amount': 0.0, 'ehail_fee': nan, 'improvement_surcharge': 0.3, 'total_amount': 9.05, 'payment_type': 2.0, 'trip_type': 1.0, 'congestion_surcharge': 0.0} \n",
      "3 {'VendorID': 1.0, 'lpep_pickup_datetime': '2019-10-01 00:09:31', 'lpep_dropoff_datetime': '2019-10-01 00:24:47', 'store_and_fwd_flag': 'N', 'RatecodeID': 1.0, 'PULocationID': 255, 'DOLocationID': 228, 'passenger_count': 2.0, 'trip_distance': 7.5, 'fare_amount': 21.5, 'extra': 0.5, 'mta_tax': 0.5, 'tip_amount': 0.0, 'tolls_amount': 0.0, 'ehail_fee': nan, 'improvement_surcharge': 0.3, 'total_amount': 22.8, 'payment_type': 2.0, 'trip_type': 1.0, 'congestion_surcharge': 0.0} \n"
     ]
    }
   ],
   "source": [
    "TOPIC_NAME = 'green-trips2'\n",
    "TEST_LOAD=False\n",
    "\n",
    "print(' load green trips data to df..')\n",
    "df = pd.read_csv('green_tripdata_2019-10.csv')\n",
    "\n",
    "#look at the data\n",
    "for i, row in enumerate(df.itertuples(index=False),1):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    print(i, row_dict, '')\n",
    "    if i == 3:\n",
    "        break\n",
    "##        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab68fb4",
   "metadata": {},
   "source": [
    "## check kafka connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dcbe2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check kafka connection..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check kafka connection\n",
    "\n",
    "print('check kafka connection..')\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbddf8c1",
   "metadata": {},
   "source": [
    "## load data to kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7b936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data to kafka..\n",
      "**end-end took 64.59 seconds\n"
     ]
    }
   ],
   "source": [
    "print('load data to kafka..')\n",
    "##load data to kafka\n",
    "def load_data():\n",
    "    t0 = time.time()\n",
    "    for i, row in enumerate(df.itertuples(index=False),1):\n",
    "        row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "        #print(i, row_dict, '')\n",
    "        if TEST_LOAD and i == 5:\n",
    "            break\n",
    "        message = {}    \n",
    "        message['lpep_pickup_datetime']= row_dict['lpep_pickup_datetime']\n",
    "        message['lpep_dropoff_datetime']= row_dict['lpep_dropoff_datetime']\n",
    "        message['PULocationID']= row_dict['PULocationID']\n",
    "        message['DOLocationID']= row_dict['DOLocationID']\n",
    "        message['passenger_count']= row_dict['passenger_count']\n",
    "        message['trip_distance']= row_dict['trip_distance']\n",
    "        message['tip_amount']= row_dict['tip_amount']\n",
    "        t1 = time.time()\n",
    "        producer.send(TOPIC_NAME, value=message)\n",
    "        t2 = time.time()\n",
    "        #print(f'** loop time to send {(t2 - t1):.2f} seconds for index ', i )\n",
    "        t1 = t2\n",
    "\n",
    "    producer.flush()\n",
    "    t3 = time.time()\n",
    "    print(f'**end-end took {(t3-t0):.2f} seconds')    \n",
    "    \n",
    "load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d6e4f",
   "metadata": {},
   "source": [
    "## create spark consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6650329b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create spark consumer...\n",
      ":: loading settings :: url = jar:file:/usr/local/Cellar/apache-spark/3.5.1/libexec/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/amohan/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/amohan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6fe16dba-d99d-4dc2-9e5e-db57ea5e03d5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 546ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6fe16dba-d99d-4dc2-9e5e-db57ea5e03d5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/8ms)\n",
      "24/03/30 17:47:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "##create spark consumer\n",
    "print('create spark consumer...')\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer2\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4d235",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7184d429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data in spark streaming...\n"
     ]
    }
   ],
   "source": [
    "## read data\n",
    "print('read data in spark streaming...')\n",
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", TOPIC_NAME) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8a524c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema for raw data\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      " green_stream  DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\n"
     ]
    }
   ],
   "source": [
    "print('schema for raw data')\n",
    "green_stream.printSchema()\n",
    "print(' green_stream ' ,green_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fad4b",
   "metadata": {},
   "source": [
    "## look at first record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c224485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read raw data writeStream.foreachBatch\n",
      "query1_peek(for raw data) is active?  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/30 17:52:38 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/w7/bjnp92015253vpp0xtyzp16c0000gn/T/temporary-2c912429-1ef8-4e40-8d32-9388d8716523. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/30 17:52:38 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/30 17:52:38 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Row(key=None, value=bytearray(b'{\"lpep_pickup_datetime\": \"2019-10-01 00:26:02\", \"lpep_dropoff_datetime\": \"2019-10-01 00:39:58\", \"PULocationID\": 112, \"DOLocationID\": 196, \"passenger_count\": 1.0, \"trip_distance\": 5.88, \"tip_amount\": 0.0}'), topic='green-trips2', partition=0, offset=0, timestamp=datetime.datetime(2024, 3, 30, 17, 38, 8, 323000), timestampType=0)\n",
      "query1_peek(for raw data) is still active?  False\n"
     ]
    }
   ],
   "source": [
    "def my_peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(1)\n",
    "    if first_row:\n",
    "        print(batch_id, first_row[0])\n",
    "    #end of func\n",
    "\n",
    "print('read raw data writeStream.foreachBatch')\n",
    "query1_peek = green_stream.writeStream.foreachBatch(my_peek).start()\n",
    "print('query1_peek(for raw data) is active? ',query1_peek.isActive)\n",
    "time.sleep(30)\n",
    "query1_peek.stop()\n",
    "print('query1_peek(for raw data) is still active? ',query1_peek.isActive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107631a",
   "metadata": {},
   "source": [
    "## adding schema to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7253d847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add schema to data...\n",
      "schema after attaching schema to data\n",
      "root\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('add schema to data...')\n",
    "### add schema to data\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())\n",
    "\n",
    "green_stream_withschema = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")\n",
    "\n",
    "print('schema after attaching schema to data')\n",
    "green_stream_withschema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0641e",
   "metadata": {},
   "source": [
    "## look at the data again (with schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb9eb9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read raw data writeStream.foreachBatch (with schema)\n",
      "query2_peek (with schema) is active?  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/30 17:56:54 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/w7/bjnp92015253vpp0xtyzp16c0000gn/T/temporary-8f17cccc-fa81-497e-ab1a-2a7b497c0598. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/30 17:56:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/30 17:56:54 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Row(lpep_pickup_datetime='2019-10-01 00:26:02', lpep_dropoff_datetime='2019-10-01 00:39:58', PULocationID=112, DOLocationID=196, passenger_count=1.0, trip_distance=5.88, tip_amount=0.0)\n",
      "query2_peek (with schema) is still active?  False\n"
     ]
    }
   ],
   "source": [
    "print('read raw data writeStream.foreachBatch (with schema)')\n",
    "query2_peek = green_stream_withschema.writeStream.foreachBatch(my_peek).start()\n",
    "print('query2_peek (with schema) is active? ',query2_peek.isActive)\n",
    "time.sleep(30)\n",
    "query2_peek.stop()\n",
    "print('query2_peek (with schema) is still active? ',query2_peek.isActive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e2d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
